# -*- coding: utf-8 -*-
"""enrollment_connect_Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11YijFosY9P8TCfUjN2qIMKGC8eLCnS8x
"""

import pandas as pd 
#import findspark
#findspark.init('/opt/spark')
import pyspark
import random
from pyspark import SparkContext, SparkConf, SQLContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf 
from pyspark.sql.types import StructField,IntegerType, StructType,StringType, FloatType
from pyspark.sql.functions import desc 
from pyspark.sql.functions import asc 
import pyspark.sql.functions as F 
import numpy as np
from pyspark.sql import Window
from pyspark.sql.functions import isnan, when, count, col
from pyspark.sql.types import DateType
from pyspark.sql.functions import *

# Commented out IPython magic to ensure Python compatibility.
# %sql
Show databases;
use databricks_metastore

# Commented out IPython magic to ensure Python compatibility.
# %python

registry = spark.sql("select * from databricks_metastore.nbg_enrollment_metrics_ban2401g000303_registry")
studysum = spark.sql("select * from databricks_metastore.nbg_enrollment_metrics_ban2401g000303_studysum")
a3blinded_prod = spark.sql("select * from databricks_metastore.nbg_enrollment_metrics_ban2401g000303_a3blinded_prod_20220105020004")
a45blinded_prod = spark.sql("select * from databricks_metastore.nbg_enrollment_metrics_ban2401g000303_a45blinded_prod_20220105023004")

df = spark.sql('select * from databricks_metastore.nbg_enrollment_metrics_ban2401g000303_registry limit 100')
df.display()

# show the column names

registry.columns

# getting the list of column names
#col = registry.columns
 
# printing
#print(f'List of column names: {col}')
 
# visualizing the dataframe
#registry.show()

studysum.columns

a3blinded_prod.columns




#https://medium.com/@zhlli/data-wrangling-pandas-vs-pyspark-dataframe-d625b0b99c73

a45blinded_prod.columns

#https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html

# PySpark for pandas

# Display the Schema of the DataFrame
print(registry.printSchema())

#Show the Head of the DataFrame

#registry.show(5)

#registry.take(5)

#Select Columns from the DataFrame

#registry.select('subject_label')

#Show the Statistics of the py DataFrame

#registry.describe().show()

# 
# *****************************************************************************
# subjects1: “subject_label” variable for studysum table or registry table
# 
# 
# non-missing exam date2: from registry table where variable dd_field.name= “examdate” 
# and variable dd_revision_field.translated_value has non-missing value
# 
# 
# exam date3: from registry table where variable dd_field.name= “examdate”
# 
# visit4: “event.label” from registry table 
# 
# 
# disposition status5: from registry table where variable dd_field.name= “sdstatus” and 
# variable dd_revision_field.translated_value = “Never randomized”
# 
# subjects6: “PARTICIPANT ID” variable for a3blinded_prod table or a45blinded_prod table
# ******************************************************************************

# 1.	Consented: Unique subjects1 from either registry table OR studysum table
# Screen Fail pre-SV1: Unique subjects1 from studysum table that DO NOT have any non-missing exam date2 from the registry table.
# 

#import os
#print(os.getcwd())

#os.listdir('/databricks/driver')

import pandas as pd 

# Convert Spark Nested Struct DataFrame to Pandas

# https://sparkbyexamples.com/pyspark/convert-pyspark-dataframe-to-pandas/
# selected varables for the demonstration
col1 = ['subject_label']
df1 = registry.select(col1)
# df1.show()
df2 = studysum.select(col1)
df1 = df1.toPandas()
df2 = df2.toPandas()
df3 = [df1, df2]
Consented = pd.concat(df3).drop_duplicates()
Consented



#type(df1)
#df1 <- registry %>% select(`Subject Label`) 
#df2 <- studysum %>% select(`Subject Label`)
#df3 <- union(df1, df2) %>% distinct()
#df3
#names(df3)

#Consented.write.saveAsTable(Consented) 

#Consented.DataFrameWriter.saveAsTable(Consented)

# PySpark

# https://towardsdatascience.com/pandas-to-pyspark-in-6-examples-bd8ab825d389
# Screen_Fail_pre_SV1

from pyspark.sql import functions as F

df1 = registry.filter(
    (F.col('dd_field_name') == 'examdate') &
    (F.col('dd_revision_field_translated_value') != '')
)



df2 = studysum 
col1 = ['subject_label']
df1 = registry.select(col1).toPandas()
#df1.show()
df3 = df2.select(col1).toPandas()

#df4 <-df3[!(df3$`Subject Label` %in% df1$`Subject Label`), ] 


#array = [1, 2, 3]
#df3.filter(df3.isin(df1) == False)


#df3[df3.subject_label isin (df1.subject_label), ]

Screen_Fail_pre_SV1= df3[~df3.subject_label.isin(df1.subject_label)].drop_duplicates()



Screen_Fail_pre_SV1
# .filter(registry.dd_field_name == "examdate" & registry.dd_revision_field_translated_value).collect()

# 2.	SV1
# Completed: Unique subjects1 who have completed the visit4 “Screening - Stage 1” with
#a non-missing exam date3 in the registry table.

# Pending: subjects1 who have visit4 “Screening - Stage 1” when
#the subjects1 have the maximum exam date3 AND the subjects1 are not present in the studysum table 


# Screen Fail: subjects1 who have visit4 “Screening - Stage 1” when the subjects1 have
# the maximum exam date3 AND the subjects1 are present in the studysum table with disposition status5 ‘Never randomized’
# 
# \

#Consented

from pyspark.sql import functions as F

df1 = registry.filter(
    (F.col('dd_field_name') == 'examdate') & (F.col('event_label') == 'Screening - Stage 1') & 
    (F.col('dd_revision_field_translated_value') != '')
)

SV1_Completed = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()

SV1_Completed

from pyspark.sql import functions as F

df1 = registry.filter(
    (F.col('dd_field_name') == 'examdate') & (F.col('event_label') == 'Screening - Stage 1') & 
    (F.col('dd_revision_field_translated_value') != '')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate'))

df2 = Consented[Consented.subject_label.isin(df1.toPandas().
                                                       subject_label)].drop_duplicates()


df3 = studysum.toPandas()

df4 = Consented[~Consented.subject_label.isin(df3.subject_label)]

df5 = pd.merge(df2, df4,on='subject_label')


SV1_Pending = df5

SV1_Pending

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 1') &
                      (F.col('dd_revision_field_translated_value') != '')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate'))

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()




df3 = studysum.filter((F.col('dd_field_name') == 'sdstatus') & (F.col('dd_revision_field_translated_value') == 'Never randomized') ).select('subject_label')




df4 = Consented[Consented.subject_label.isin(df3.toPandas().subject_label)].drop_duplicates()




df5 = pd.merge(df2, df4, on = 'subject_label').drop_duplicates()

SV1_Screen_Fail = df5


SV1_Screen_Fail

# 
# 
# 
# 3.	SV2
# Completed: Unique subjects1 who have completed the visit4 “Screening - Stage 2” with a non-missing exam date3 in the registry table.
# Pending: subjects1 who have visit4 “Screening - Stage 2” when the subjects1 have the maximum exam date3 AND the subjects1 are not present in the studysum table 
# Screen Fail: subjects1 who have visit4 “Screening - Stage 2” when the subjects1 have the maximum exam date3 AND the subjects1 are present in the studysum table 
# with disposition status5 ‘Never randomized’
# 
# 
# 
# 
# 
from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 2') &
                      (F.col('dd_revision_field_translated_value') != ''))


df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()

SV2_Completed = df2

SV2_Completed

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 2') &
                      (F.col('dd_revision_field_translated_value') != '')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate')).dropDuplicates()

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()



df3= studysum.toPandas()


df4 = Consented[~Consented.subject_label.isin(df3.subject_label)].drop_duplicates()

df5 =  pd.merge(df2, df4, how = 'inner',on = 'subject_label')


SV2_Pending = df5.drop_duplicates()

SV2_Pending

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 2') &
                      (F.col('dd_revision_field_translated_value') != ' ')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate')).dropDuplicates()

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()



df3 = studysum.filter((F.col('dd_field_name') == 'sdstatus') &
                      (F.col('dd_revision_field_translated_value') == 'Never randomized')).dropDuplicates()


df4 = Consented[Consented.subject_label.isin(df3.toPandas().subject_label)].drop_duplicates()

df5 =  pd.merge(df2, df4, how='inner', on = 'subject_label')


SV2_Screen_Fail = df5.drop_duplicates()

SV2_Screen_Fail

# 
# 
# 
# 4.	SV3
# Completed: Unique subjects1 who have completed the visit4 “Screening - Stage 3” with a non-missing exam date3 in the registry table.
# Pending: subjects1 who have visit4 “Screening - Stage 3” when the subjects1 have the maximum exam date3 AND the subjects1 are not present in the studysum table 
# Screen Fail: subjects1 who have visit4 “Screening - Stage 3 when the subjects1 have the maximum exam date3 AND the subjects1 are present in the studysum table with disposition status5 ‘Never randomized’
# 
# 
# 
# 



from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 3') &
                      (F.col('dd_revision_field_translated_value') != ' ')).dropDuplicates()

SV3_Completed = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()


SV3_Completed

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 3') &
                      (F.col('dd_revision_field_translated_value') != ' ')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate')).dropDuplicates()

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()



df3 = studysum

df4 = Consented[~Consented.subject_label.isin(df3.toPandas().subject_label)]

df5 =  pd.merge(df2, df4, how='inner', on = 'subject_label')


SV3_Pending = df5.drop_duplicates()

SV3_Pending

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 3') &
                      (F.col('dd_revision_field_translated_value') != ' ')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate')).dropDuplicates()

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()



df3 = studysum.filter((F.col('dd_field_name') == 'sdstatus') &
                      (F.col('dd_revision_field_translated_value') == 'Never randomized')).dropDuplicates()


df4 = Consented[Consented.subject_label.isin(df3.toPandas().subject_label)].drop_duplicates()

df5 =  pd.merge(df2, df4, how='inner', on = 'subject_label')


SV3_Screen_Fail = df5.drop_duplicates()

SV3_Screen_Fail

# 
# 5.	SV4
# Completed: Unique subjects1 who have completed the visit4 “Screening - Stage 4” with a non-missing exam date3 in the registry table.
# Pending: subjects1 who have visit4 “Screening - Stage 4” when the subjects1 have the maximum exam date3 AND the subjects1 are not present in the studysum table 
# Screen Fail: subjects1 who have visit4 “Screening - Stage 4 when the subjects1 have the maximum exam date3 AND the subjects1 are present in the studysum table with disposition status5 ‘Never randomized’
# 




from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 4') &
                      (F.col('dd_revision_field_translated_value') != ' ')).dropDuplicates()

SV4_Completed = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()


SV4_Completed

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 4') &
                      (F.col('dd_revision_field_translated_value') != ' ')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate')).dropDuplicates()

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()



df3 = studysum.dropDuplicates()


df4 = Consented[~Consented.subject_label.isin(df3.toPandas().subject_label)].drop_duplicates()

df5 =  pd.merge(df2, df4, how='inner', on = 'subject_label')


SV4_Pending  = df5.drop_duplicates()

SV4_Pending

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 4') &
                      (F.col('dd_revision_field_translated_value') != ' ')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate')).dropDuplicates()

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()



df3 = studysum.filter((F.col('dd_field_name') == 'sdstatus') &
                      (F.col('dd_revision_field_translated_value') == 'Never randomized')).dropDuplicates()


df4 = Consented[Consented.subject_label.isin(df3.toPandas().subject_label)].drop_duplicates()

df5 =  pd.merge(df2, df4, how='inner', on = 'subject_label')


SV4_Screen_Fail = df5.drop_duplicates()

SV4_Screen_Fail

# 
# 6.	SV5
# Completed: Unique subjects1 who have completed the visit4 “Screening - Stage 5” with a non-missing exam date3 in the registry table.
# Pending: subjects1 who have visit4 “Screening - Stage 5” when the subjects1 have the maximum exam date3 AND the subjects1 are not present in the studysum table 
# Screen Fail: subjects1 who have visit4 “Screening - Stage 5 when the subjects1 have the maximum exam date3 AND the subjects1 are present in the studysum table with disposition status5 ‘Never randomized’
# 
from pyspark.sql import functions as F
df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 5') &
                      (F.col('dd_revision_field_translated_value') != ' ')).dropDuplicates()
SV5_Completed = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()
SV5_Completed

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 5') &
                      (F.col('dd_revision_field_translated_value') != ' ')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate')).dropDuplicates()

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()



df3 = studysum.dropDuplicates()


df4 = Consented[~Consented.subject_label.isin(df3.toPandas().subject_label)].drop_duplicates()

df5 =  pd.merge(df2, df4, how='inner', on = 'subject_label')


SV5_Pending = df5.drop_duplicates()

SV5_Pending

from pyspark.sql import functions as F


df1 = registry.filter((F.col('dd_field_name') == 'examdate') &
                      (F.col('event_label') == 'Screening - Stage 5') &
                      (F.col('dd_revision_field_translated_value') != ' ')).groupBy('subject_label').agg(F.max('dd_revision_field_translated_value').alias('max_examdate')).dropDuplicates()

df2 = Consented[Consented.subject_label.isin(df1.toPandas().subject_label)].drop_duplicates()



df3 = studysum.filter((F.col('dd_field_name') == 'sdstatus') &
                      (F.col('dd_revision_field_translated_value') == 'Never randomized')).dropDuplicates()


df4 = Consented[Consented.subject_label.isin(df3.toPandas().subject_label)].drop_duplicates()

df5 =  pd.merge(df2, df4, how='inner', on = 'subject_label')


SV5_Screen_Fail = df5.drop_duplicates()

SV5_Screen_Fail

# 
# 7.	Randomized: Unique subjects6 from either a3blinded_prod table or a45blinded_prod table
# Pending Randomization: (<SV5>.Completed minus <SV5>.Screen Fail minus Randomized)
# 
# Note: <SV5> data from step 6
# 
# 
# 
import pandas as pd 

# Convert Spark Nested Struct DataFrame to Pandas

# https://sparkbyexamples.com/pyspark/convert-pyspark-dataframe-to-pandas/
# selected varables for the demonstration
col1 = ['participant_id']
df1 = a3blinded_prod.select(col1)
#df1.show()
df2 = a45blinded_prod.select(col1)
df1 = df1.toPandas()
df2 = df2.toPandas()
df3 = [df1, df2]
Randomized = pd.concat(df3).drop_duplicates()
#Randomized.rename(columns={"subject_label": "subject_label"})

Randomized.rename({'participant_id': 'subject_label'}, axis=1, inplace=True)
Randomized

# df1 <- SV5_Completed[!(SV5_Completed$`Subject Label` %in% SV5_Screen_Fail$`Subject Label` )]

from pyspark.sql import functions as F

PendingRandomization = SV5_Completed[~SV5_Completed.subject_label.isin(SV5_Screen_Fail.subject_label)]

PendingRandomization

# Commented out IPython magic to ensure Python compatibility.
# show all the varaibles 

# %whos

# Commented out IPython magic to ensure Python compatibility.

# %sql 
use enrollment_abba303
-- use the database to write my tables 

-- three ways to write table in PySpark

--https://towardsdatascience.com/3-ways-to-create-tables-with-apache-spark-32aed0f355ab

type(Consented)

Consented

Category1 = ['Consented']*len(Consented)
Consented['Category'] = Category1

Category2 = ['Screen_Fail_pre_SV1']*len(Screen_Fail_pre_SV1)
Screen_Fail_pre_SV1['Category'] = Category2




Category3 = ['SV1_Completed']*len(SV1_Completed)
SV1_Completed['Category'] = Category3


Category4 = ['SV1_Pending']*len(SV1_Pending)
SV1_Pending['Category'] = Category4

Category5 = ['SV1_Screen_Fail']*len(SV1_Screen_Fail)
SV1_Screen_Fail['Category'] = Category5


Category6 = ['SV2_Completed']*len(SV2_Completed)
SV2_Completed['Category'] = Category6


Category7 = ['SV2_Pending']*len(SV2_Pending)
SV2_Pending['Category'] = Category7

Category8 = ['SV2_Screen_Fail']*len(SV2_Screen_Fail)
SV2_Screen_Fail['Category'] = Category8



Category9 = ['SV3_Completed']*len(SV3_Completed)
SV3_Completed['Category'] = Category9


Category10 = ['SV3_Pending']*len(SV3_Pending)
SV3_Pending['Category'] = Category10

Category11 = ['SV3_Screen_Fail']*len(SV3_Screen_Fail)
SV3_Screen_Fail['Category'] = Category11



Category12 = ['SV4_Completed']*len(SV4_Completed)
SV4_Completed['Category'] = Category12


Category13 = ['SV4_Pending']*len(SV4_Pending)
SV4_Pending['Category'] = Category13

Category14 = ['SV4_Screen_Fail']*len(SV4_Screen_Fail)
SV4_Screen_Fail['Category'] = Category14


Category15 = ['SV5_Completed']*len(SV5_Completed)
SV5_Completed['Category'] = Category15


Category16 = ['SV5_Pending']*len(SV5_Pending)
SV5_Pending['Category'] = Category16

Category17 = ['SV5_Screen_Fail']*len(SV5_Screen_Fail)
SV5_Screen_Fail['Category'] = Category17

Category18 = ['PendingRandomization']*len(PendingRandomization)
PendingRandomization['Category'] = Category18

Category19 = ['Randomized']*len(Randomized)
Randomized['Category'] = Category19

SV_all = pd.concat([Consented,
Screen_Fail_pre_SV1,
SV1_Completed,
SV1_Pending,
SV1_Screen_Fail,
SV2_Completed,
SV2_Pending,
SV2_Screen_Fail,
SV3_Completed,
SV3_Pending,
SV3_Screen_Fail,
SV4_Completed,
SV4_Pending,
SV4_Screen_Fail,
SV5_Completed,
SV5_Pending,
SV5_Screen_Fail,
Randomized,
PendingRandomization])


#SparkDF_SV_all=spark.createDataFrame(SV_all)

import pyspark

from pyspark.sql import SparkSession

import pandas as pd

spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()

#pdDF = pd.read_csv("samp.csv")

from pyspark.sql.types import *

mySchema = StructType([StructField('subject_label', StringType(), True)\

                       ,StructField('Category', StringType(), True)])

df_SV_all = spark.createDataFrame(SV_all, schema=mySchema)


df_SV_all.display()

# Commented out IPython magic to ensure Python compatibility.
# %sql 
use enrollment_abba303

df_SV_all_tb = spark.sql("select * from enrollment_abba303.df_SV_all")
df_SV_all_tb.display()

import pyspark

from pyspark.sql.functions import to_date, col

df= (spark.read.format('csv')\
.option('header', 'True')\
.option('inferSchema', 'true')\
.load('dbfs:/FileStore/tables/SV_all.csv'))

import pyspark

from pyspark.sql.functions import to_date, col

df = spark.read.format('csv').option('header', 'True').option('inferSchema', 'true').load('dbfs:/FileStore/tables/sMRI_unnormalised_Screening_Failure.csv')

df_SV_all.write.mode("overwrite").saveAsTable("df_SV_all")

#OTHER DATA SOURCES FOR MANAGED & UNMANAGED TABLES

#sparkDF.createOrReplaceTempView('df_SV_all') #source_2

#%sql
#CREATE TABLE IF NOT EXISTS SV_all_tb2 AS 
#SELECT * FROM df_SV_all

#spark.sql(“CREATE TABLE IF NOT EXISTS salesTable_manag2 AS SELECT * FROM df_final_View”)



#METHOD 1 - Managed Table
#sparkDF.write.mode("overwrite").saveAsTable("PendingRandomization_tb1")

#spark.catalog.listTables()

#sparkDF.repartition(2).write.mode("overwrite").save("/FileStore/PendingRandomization_View.parquet") #source_3

# Commented out IPython magic to ensure Python compatibility.
# %whos

# Commented out IPython magic to ensure Python compatibility.
# show the paths

# %fs ls /

# read data from the path

#import pyspark
#from pyspark.sql.functions import to_date, col

#LOAD CSV FILE - INCLUDES 5M ROWS
#df = (spark.read.format('csv')
#           .option('header', 'True')
#           .option("inferSchema", "true")
#           .load('/FileStore/tables/SV_all.csv'))

#%sql 
#use enrollment_abba303

#%sql 

#create table enrollment_abba303.test as 
#SELECT 'testdata' as dummyv

#spark.sql("CREATE DATABASE learn_spark_db")

#studysum.show()